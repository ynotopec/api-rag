# OpenAIâ€‘compatible RAG API

Un serveur **FastAPI** exposant une API **compatible OpenAI** (`/v1/chat/completions`) dÃ©diÃ©e au **RAG conversationnel** (retrievalâ€‘augmented generation) basÃ© sur **FAISS** + **HuggingFace embeddings**.

> Fichier principalÂ : `app.py` â€” Version APIÂ : **2.1.0**

---

## âœ¨ FonctionnalitÃ©s

* **CompatibilitÃ© OpenAI**: accepte les payloads `chat/completions` standards
* **ModÃ¨le logique unique** exposÃ©Â : `ai-rag`
* **RAG conversationnel** avec stratÃ©gie **rewrite + HyDE** (configurable)
* **FAISS** persistant via `vectorstore_db/` (autoâ€‘construction si `wiki.txt` est prÃ©sent)
* **RÃ©Ã©criture de requÃªte** et **HyDE** (configurables) pour amÃ©liorer le rappel documentaire
* **Recherche hybride FAISS + BM25** (activÃ©e par dÃ©faut, configurable)
* **Fusion RRF + dÃ©duplication + reranking** des chunks (optionnels)
* **Sources** renvoyÃ©es dans la rÃ©ponse (suffixe Â«Â Sources: â€¦Â Â»)
* **Streaming SSE** relayÃ© depuis lâ€™amont (chunks OpenAI)
* **Auth Bearer** optionnelle cÃ´tÃ© entrÃ©e, **clÃ© OpenAI** cÃ´tÃ© sortie

---

## ğŸ§  Algorithme RAG (vue dâ€™ensemble)

```text
[Messages] â†’ (1) RÃ©Ã©criture â†’ q'
             (2) HyDE (pseudoâ€‘document) â†’ h
             (3) Retrieval FAISS (similarity_search)
                 â€¢ q (immÃ©diat)
                 â€¢ q' (si activÃ©)
                 â€¢ h (si activÃ©)
             (4) Fusion RRF + dÃ©duplication (+ reranking)
             (5) Contexte = concat topâ€‘K (K=RAG_TOP_K)
             (6) Prompt vers modÃ¨le amont (UPSTREAM_MODEL_RAG)
             (7) RÃ©ponse + liste des sources
```

### Diagramme de flux

```mermaid
flowchart LR
    client((Client)) --> req["POST /v1/chat/completions"]
    req --> auth{Token valide ?}
    auth -->|non| err[401]
    auth -->|oui| dispatch{ModÃ¨le demandÃ©}

    subgraph rag[Pipeline RAG]
        direction TB
        ensure["Chargement/MAJ index<br/>FAISS + BM25"]
        classify["Classification rapide<br/>RAG vs CHAT (optionnelle)"]
        original["Recherche immÃ©diate<br/>similarity_search(q)<br/>+ BM25 (optionnel)"]
        rewrite["RÃ©Ã©criture requÃªte (q')<br/>UPSTREAM_MODEL_REWRITE"]
        hyde["HyDE (h)<br/>pseudo-document"]
        search_rewrite["Recherche q'"]
        search_hyde["Recherche h"]
        fusion["Fusion RRF"]
        dedup["DÃ©duplication rapide"]
        rerank["Reranking CrossEncoder<br/>(optionnel)"]
        topk["SÃ©lection topâ€‘K + formatage"]
        prompt["Contexte + historique<br/>â†’ prompt final"]
        rag_call["Appel UPSTREAM_MODEL_RAG"]
        fallback["Fallback chat (sans contexte)"]
        ensure --> classify
        classify -->|CHAT| fallback
        classify -->|RAG| original
        original --> fusion
        rewrite --> search_rewrite
        hyde --> search_hyde
        search_rewrite --> fusion
        search_hyde --> fusion
        fusion --> dedup --> rerank --> topk --> prompt --> rag_call
    end

    dispatch -->|ai-rag| ensure
    rag_call --> format
    fallback --> format
    format --> stream["Streaming SSE relayÃ©<br/>ou JSON"]
    stream --> client
```

### DÃ©tails Ã©tape par Ã©tape

0. **Vectorstore** (`_ensure_vectorstore`)Â : charge `vectorstore_db/` (index FAISS + `chunks.pkl`), le reconstruit si absent, si `RAG_FORCE_REBUILD=on`, ou si les sources dâ€™ingestion sont plus rÃ©centes.
1. **Historique court**Â : un extrait des 3 derniers messages prÃ©cÃ©dents est construit pour la rÃ©Ã©criture.
2. **RÃ©Ã©criture** (`_rewrite_query`)Â : le serveur appelle le LLM amont (paramÂ `UPSTREAM_MODEL_REWRITE`) pour produire une requÃªte optimisÃ©e.
3. **HyDE** (`_hyde_expand`)Â : on gÃ©nÃ¨re un pseudoâ€‘document court Ã  partir de la requÃªte utilisateur.
4. **Classification (optionnelle)**Â : un filtre rapide **RAG vs CHAT** peut forcer un fallback sans contexte si activÃ© (`ENABLE_QUERY_CLASSIFICATION=true`).
5. **Retrieval** (`_retrieve_pipeline`)Â :

   * **Recherche immÃ©diate** sur la requÃªte utilisateur.
   * **Recherche secondaire** sur la requÃªte rÃ©Ã©crite (si activÃ©e).
   * **Recherche HyDE** sur le pseudoâ€‘document (si activÃ©e).
   * **Hybrid BM25** optionnel en complÃ©ment.
6. **Fusion & dÃ©duplication**Â : fusion RRF, dÃ©duplication (hash + fuzzy match), reranking optionnel.
7. **Contexte**Â : concatÃ©nation des `RAG_TOP_K` premiers chunks.
8. **GÃ©nÃ©ration**Â : on envoie au modÃ¨le amont un prompt systÃ¨me + le **contexte** + lâ€™historique rÃ©cent.
9. **Sources**Â : noms de fichiers (mÃ©tadonnÃ©e `source`) dÃ©duits des chunks retenus.

> âš ï¸ Si aucun chunk pertinentÂ : rÃ©ponse courte indiquant lâ€™insuffisance du contexte.

---

## ğŸ—ï¸ Architecture interne

* **FastAPI** + **CORSMiddleware**
* **/v1/chat/completions**Â : route unique cÃ´tÃ© client
* **RAG**Â : construit un prompt enrichi par le contexte, puis appelle `UPSTREAM_MODEL_RAG`
* **FAISS**Â : persistance sur disqueÂ ; reâ€‘binding de la fonction dâ€™embedding au chargement
* **Embeddings**Â : `BAAI/bge-m3` (HuggingFace)
* **Rebuild conditionnel**Â : recharge `vectorstore_db/` ou le rÃ©gÃ©nÃ¨re si `wiki.txt` est plus rÃ©cent ou si `RAG_FORCE_REBUILD` est activÃ©

---

## ğŸ”§ Variables dâ€™environnement

| Variable | DÃ©faut | Description |
| --- | --- | --- |
| `OPENAI_API_BASE` | `http://localhost:8000/v1` | Endpoint OpenAIâ€‘compatible en amont. |
| `OPENAI_API_KEY` | `changeme` | ClÃ© API pour lâ€™amont. |
| `API_AUTH_TOKEN` | *(vide)* | Si dÃ©finiÂ : **obligatoire** en entrÃ©e via `Authorization: Bearer â€¦`. |
| `UPSTREAM_MODEL_RAG` | `gpt-4o-mini` | ModÃ¨le amont pour la gÃ©nÃ©ration RAG. |
| `UPSTREAM_MODEL_REWRITE` | `=UPSTREAM_MODEL_RAG` | ModÃ¨le amont pour la rÃ©Ã©criture et HyDE. |
| `MODEL_RAG` | `ai-rag` | Nom logique exposÃ© pour le pipeline RAG. |
| `VECTORSTORE_DIR` | `vectorstore_db` | Dossier du FAISS sÃ©rialisÃ© (persistÃ© sur disque). |
| `WIKI_TXT` | `wiki.txt` | Corpus brut utilisÃ© pour construire le FAISS si absent. |
| `INGESTION_SOURCES` | `text` | Sources dâ€™ingestion activÃ©es (`text`, `thunderbird`, ou liste sÃ©parÃ©e par des virgules). |
| `INGESTION_TEXT_PATHS` | *(vide)* | Liste de fichiers ou dossiers `.txt` (sÃ©parÃ©s par virgule). Si vide, utilise `WIKI_TXT`. |
| `THUNDERBIRD_PROFILE_DIR` | *(vide)* | Chemin du profil Thunderbird local (Windows : `%APPDATA%\\Thunderbird\\Profiles\\<profil>`). |
| `THUNDERBIRD_MAX_MESSAGES` | `10000` | Limite max de messages ingÃ©rÃ©s depuis Thunderbird. |
| `INGESTION_REFRESH_INTERVAL` | `0` | Intervalle (secondes) pour rebalayer les sources et reconstruire lâ€™index si besoin. |
| `RAG_FORCE_REBUILD` | `true` | Si `1/true/on`Â : force la reconstruction du FAISS au dÃ©marrage. |
| `RAG_TOP_K` | `8` | Nombre max de chunks concatÃ©nÃ©s dans le contexte. |
| `RAG_QUERY_STRATEGY` | `rewrite+hyde` | `simple`, `rewrite`, `hyde` ou `rewrite+hyde`. |
| `RAG_HISTORY_WINDOW` | `6` | Nb. de messages conservÃ©s pour le prompt final. |
| `ENABLE_HYBRID_SEARCH` | `true` | Active la recherche BM25 hybride. |
| `ENABLE_RERANKING` | `true` | Active le reranking CrossEncoder. |
| `ENABLE_QUERY_CLASSIFICATION` | `true` | Active la classification rapide RAG vs CHAT. |
| `EMBEDDING_MODEL` | `BAAI/bge-m3` | ModÃ¨le dâ€™embeddings utilisÃ©. |
| `BM25_K` | `4` | Nombre de rÃ©sultats BM25 pris en compte. |
| `PORT` | `8080` | Port HTTP local. |

---

## ğŸš€ DÃ©marrage rapide

### PrÃ©requis

* PythonÂ 3.10+
* `pip` (ou `uv`, `poetry` au choix)

### Installation

```bash
python -m venv .venv && source .venv/bin/activate
pip install -U pip
pip install fastapi uvicorn[standard] requests langchain-community langchain-huggingface langchain-text-splitters faiss-cpu pydantic
# ou, pour rester alignÃ© avec ce dÃ©pÃ´tÂ :
pip install -r requirements.txt
```

> **FAISS**Â : selon lâ€™OS, vous pouvez prÃ©fÃ©rer `faiss-gpu`.

### DonnÃ©es

* Par dÃ©faut, placez vos contenus dans `wiki.txt` (texte brut). Au premier dÃ©marrage sans `vectorstore_db/`, lâ€™index sera construit et persistÃ©.
* Pour plusieurs fichiers/dossiers texteÂ : utilisez `INGESTION_TEXT_PATHS="/chemin/a.txt,/chemin/vers/dossier"` et `INGESTION_SOURCES=text`.
* Pour ingÃ©rer des mails Thunderbird locauxÂ : dÃ©finissez `INGESTION_SOURCES=text,thunderbird` et `THUNDERBIRD_PROFILE_DIR` vers le dossier de profil.
* Pour ajouter rÃ©guliÃ¨rement les nouveaux mailsÂ : dÃ©finissez `INGESTION_REFRESH_INTERVAL=300` (ex. toutes les 5 minutes).

### Lancer le serveur

```bash
export OPENAI_API_BASE="https://api.openai.com/v1"           # ou votre passerelle vLLM
export OPENAI_API_KEY="sk-..."
export API_AUTH_TOKEN="my-inbound-token"
uvicorn app:app --host 0.0.0.0 --port 8080
```

### Lancer via `run.sh` (avec `.env`)

Le script `run.sh` crÃ©e un venv dÃ©diÃ©, installe les dÃ©pendances et lance `uvicorn`. Il **nÃ©cessite** un fichier `.env` Ã  la racine avec les variables dâ€™environnement (au minimum `OPENAI_API_BASE` et `OPENAI_API_KEY`).

```bash
# Exemple minimal .env
OPENAI_API_BASE="https://api.openai.com/v1"
OPENAI_API_KEY="sk-..."
API_AUTH_TOKEN="my-inbound-token"
```

Puis dÃ©marrez avecÂ :

```bash
./run.sh 0.0.0.0 8080
```

---

## ğŸ§ª Appels API (exemples)

### 1) RAG (`MODEL_RAG`)

```bash
curl -s http://localhost:8080/v1/chat/completions \
  -H "Authorization: Bearer my-inbound-token" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "ai-rag",
    "messages": [
      {"role": "user", "content": "RÃ©sume les points clÃ©s du document sur la migration Kubernetes."}
    ]
  }'
```

### 2) Streaming (SSE relayÃ© cÃ´tÃ© serveur)

```bash
curl -N http://localhost:8080/v1/chat/completions \
  -H "Authorization: Bearer my-inbound-token" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "ai-rag",
    "stream": true,
    "messages": [
      {"role": "user", "content": "Donne une synthÃ¨se en 6 lignes."}
    ]
  }'
```

> En mode `stream: true`, le serveur relaie le flux SSE de lâ€™amont.

---

## âœ… Ã‰valuer la qualitÃ© du RAG

Ce dÃ©pÃ´t fournit un script lÃ©ger pour mesurer la qualitÃ© du RAG **sans dÃ©pendances externes**. Le principeÂ : envoyer une liste de questions, vÃ©rifier la prÃ©sence des **sources** renvoyÃ©es par lâ€™API, et mesurer si la rÃ©ponse contient des **motsâ€‘clÃ©s attendus**.

### 1) PrÃ©parer un dataset JSONL

Chaque ligne contientÂ :
* `question`Â : la question utilisateur.
* `expected_sources`Â : liste de sources attendues (ex. `README.md`).
* `expected_keywords`Â : liste de motsâ€‘clÃ©s attendus dans la rÃ©ponse.
* `metadata`Â : optionnel.

Un exemple est fourniÂ : `eval_dataset.sample.jsonl`.

### 2) Lancer lâ€™Ã©valuation

```bash
python evaluate_rag.py \
  --dataset eval_dataset.sample.jsonl \
  --endpoint http://localhost:8080/v1/chat/completions \
  --model ai-rag \
  --token my-inbound-token
```

Le script affiche un rÃ©sumÃ© JSONÂ avecÂ :
* `avg_precision`Â : prÃ©cision des sources.
* `avg_recall`Â : rappel des sources.
* `avg_keyword_recall`Â : rappel des motsâ€‘clÃ©s.

### 3) Exporter les rÃ©sultats dÃ©taillÃ©s

```bash
python evaluate_rag.py \
  --dataset eval_dataset.sample.jsonl \
  --output eval_results.json
```

> ğŸ’¡ AstuceÂ : enrichissez le dataset avec vos documents rÃ©els (noms de fichiers attendus) pour une mesure fidÃ¨le.

---

## ğŸ” Authentification

* **EntrÃ©e**Â : si `API_AUTH_TOKEN` est dÃ©fini, chaque requÃªte **doit** fournir `Authorization: Bearer <token>`.
* **Sortie**Â : `OPENAI_API_KEY` est utilisÃ© cÃ´tÃ© serveur pour appeler le fournisseur amont.

---

## ğŸ—ƒï¸ Indexation & embeddings

* EmbeddingsÂ : `BAAI/bge-m3`
* SplitterÂ : `RecursiveCharacterTextSplitter(chunk_size=800, overlap=100)`
* PersistantÂ : `vectorstore_db/` (`index.faiss` + `chunks.pkl`)

---

## âš™ï¸ ParamÃ©trage RAG

* **StratÃ©gies** (`RAG_QUERY_STRATEGY`)Â :

  * `simple`Â : pas de rÃ©Ã©criture ni HyDE
  * `rewrite`Â : rÃ©Ã©criture seule
  * `hyde`Â : HyDE seul
  * `rewrite+hyde`Â : rÃ©Ã©criture puis HyDE (par dÃ©faut)
* **Topâ€‘K**Â : `RAG_TOP_K` (8 par dÃ©faut)
* **FenÃªtre dâ€™historique**Â : `RAG_HISTORY_WINDOW` (6 par dÃ©faut)

---

## ğŸ§¯ DÃ©pannage

* **401 Missing Authorization**Â : dÃ©finissez `API_AUTH_TOKEN` cÃ´tÃ© serveur et envoyez lâ€™enâ€‘tÃªte Bearer cÃ´tÃ© client.
* **Vectorstore introuvable**Â : fournissez `wiki.txt` au premier lancement, ou placez un `vectorstore_db/` existant.
* **Performances embeddings**Â : selon lâ€™OS/CPU, prÃ©fÃ©rez `faiss-gpu` si GPU dispo.
* **RÃ©ponses Â«Â contexte insuffisantÂ Â»**Â : augmentez `RAG_TOP_K`, amÃ©liorez `wiki.txt`, ou dÃ©sactivez le reranking.

---

## ğŸ”§ Conseils performance/qualitÃ©

* Nettoyez/structurez `wiki.txt` (titres, sÃ©parateurs) pour de meilleurs chunks
* Ajustez `chunk_size`/`overlap` si vos documents sont hÃ©tÃ©rogÃ¨nes
* Reâ€‘entraÃ®ner FAISS (reconstruire `vectorstore_db/`) aprÃ¨s de gros changements de corpus
* Fixez `RAG_TOPIC_PREFIX` pour forcer un domaine (ex. Â«Â KubernetesÂ Â», Â«Â DSFRÂ Â», etc.)

---

## ğŸ§­ Roadmap (suggestions)

* Support **/v1/embeddings** pour publier les embeddings
* Ajout dâ€™un **reranker** (ex. crossâ€‘encoder) facultatif
* **Citations** positionnelles (lignes/offsets dans la source)
* **Batching** upstream pour rÃ©duire la latence

---

## ğŸ“„ Licence

Ã€ dÃ©finir par le propriÃ©taire du dÃ©pÃ´t (ex. MIT, Apacheâ€‘2.0).

---

## ğŸ™Œ CrÃ©dits

* FAISS â€” Facebook AI Similarity Search
* HuggingFace Transformers & sentenceâ€‘transformers
* FastAPI

---

## Structure minimale du repo

```
.
â”œâ”€â”€ app.py
â”œâ”€â”€ wiki.txt                # optionnel, pour construire FAISS au premier run
â”œâ”€â”€ vectorstore_db/         # gÃ©nÃ©rÃ© automatiquement si absent et wiki.txt prÃ©sent
â””â”€â”€ README.md
```

# OpenAIâ€‘compatible RAG API

Un serveur **FastAPI** exposant une API **compatible OpenAI** (`/v1/chat/completions`) dÃ©diÃ©e au **RAG conversationnel** (retrievalâ€‘augmented generation) basÃ© sur **FAISS** + **HuggingFace embeddings**.

> Fichier principalÂ : `app.py` â€” Version APIÂ : **1.2.1**

---

## âœ¨ FonctionnalitÃ©s

* **CompatibilitÃ© OpenAI**: accepte les payloads `chat/completions` standards
* **ModÃ¨le logique unique** exposÃ©Â : `ai-rag`
* **RAG conversationnel** avec stratÃ©gie **rewrite + HyDE** (configurable)
* **FAISS** persistant via `vectorstore.pkl` (autoâ€‘construction si `wiki.txt` est prÃ©sent)
* **RÃ©Ã©criture de requÃªte** (FR) et **HyDE** pour amÃ©liorer le rappel documentaire
* **MMR + seuil de similaritÃ©** + **dÃ©duplication** des chunks
* **Sources** renvoyÃ©es dans la rÃ©ponse (suffixe Â«Â Sources: â€¦Â Â»)
* **Streaming SSE** simulÃ© (chunks OpenAI) cÃ´tÃ© serveur
* **Auth Bearer** optionnelle cÃ´tÃ© entrÃ©e, **clÃ© OpenAI** cÃ´tÃ© sortie
* **Patch auto du tokenizer `pad_token`** pour certains embeddings HuggingFace

---

## ğŸ§  Algorithme RAG (vue dâ€™ensemble)

```text
[Messages] â†’ (1) RÃ©Ã©criture FR (<=32 mots) â†’ q
             (2) HyDE (rÃ©ponse fictive courte) â†’ pseudo
             (3) Retrieval FAISS
                 â€¢ MMR(k=8, fetch_k=24, Î»=0.5) sur q
                 â€¢ Similarity threshold(scoreâ‰¥0.25, k=12) sur pseudo
             (4) Fusion + dÃ©duplication des chunks
             (5) Contexte = concat topâ€‘K (K=RAG_TOP_K)
             (6) Prompt vers modÃ¨le amont (UPSTREAM_MODEL_RAG)
             (7) RÃ©ponse + liste des sources
```

### Diagramme de flux

```mermaid
flowchart LR
    client((Client)) --> req["POST /v1/chat/completions"]
    req --> auth{Token valide ?}
    auth -->|non| err[401]
    auth -->|oui| dispatch{ModÃ¨le demandÃ©}

    subgraph rag[Pipeline RAG]
        direction TB
        ensure["Chargement FAISS<br/>(_ensure_vectorstore)"]
        rewrite["RÃ©Ã©criture FR<br/>UPSTREAM_MODEL_REWRITE"]
        hyde["HyDE<br/>pseudo-rÃ©ponse"]
        retriever_mmr["Retrieval MMR<br/>(k=8, fetch=24)"]
        retriever_thresh["Retrieval seuil<br/>(score â‰¥ 0.25, k=12)"]
        dedup["Fusion + dÃ©duplication"]
        prompt["Contexte + historique<br/>â†’ prompt final"]
        rag_call["Appel UPSTREAM_MODEL_RAG"]
        ensure --> rewrite
        rewrite --> retriever_mmr
        rewrite --> hyde
        hyde --> retriever_thresh
        retriever_mmr --> dedup
        retriever_thresh --> dedup
        dedup --> prompt
        prompt --> rag_call
    end

    dispatch -->|ai-rag| ensure
    rag_call --> format
    format --> stream["Streaming SSE simulÃ©<br/>ou JSON"]
    stream --> client
```

### DÃ©tails Ã©tape par Ã©tape

0. **Vectorstore** (`_ensure_vectorstore`)Â : charge `vectorstore.pkl`, le reconstruit si absent, si `RAG_FORCE_REBUILD=on`, ou si `wiki.txt` est plus rÃ©cent.
1. **FenÃªtre dâ€™historique**Â : un extrait de `HISTORY_WINDOW` derniers messages est construit.
2. **RÃ©Ã©criture** (`_rewrite_query`)Â : le serveur appelle le LLM amont (paramÂ `UPSTREAM_MODEL_REWRITE`) pour produire **une** requÃªte FR autonome (â‰¤Â 32Â mots), optionnellement prÃ©fixÃ©e par `RAG_TOPIC_PREFIX`.
3. **HyDE** (`_hyde_expand`)Â : on gÃ©nÃ¨re une **rÃ©ponse idÃ©ale courte** (FR, â‰¤Â 6Â lignes) Ã  partir de la requÃªte rÃ©Ã©crite, pour densifier la sÃ©mantique lors du retrieval.
4. **Retrieval** (`_retrieve_with_strategy`)Â :

   * **MMR** (k=8, fetch\_k=24, Î»=0.5) sur la requÃªte rÃ©Ã©crite â†’ diversitÃ© des chunks.
   * **Seuil** (scoreâ‰¥0.25, k=12) sur le texte HyDE â†’ chunks trÃ¨s pertinents.
5. **DÃ©duplication**Â : on conserve le premier chunk de chaque Â«Â signatureÂ Â» calculÃ©e sur les 256 premiers caractÃ¨res.
6. **Contexte**Â : concatÃ©nation des `RAG_TOP_K` premiers chunks.
7. **GÃ©nÃ©ration**Â : on envoie au modÃ¨le amont un prompt systÃ¨me prudent + le **contexte** + la **requÃªte utilisateur initiale** (et non la rÃ©Ã©criture). Le LLM doit **se limiter** au contexte, sinon indiquer que câ€™est insuffisant.
8. **Sources**Â : noms de fichiers (mÃ©tadonnÃ©e `source`) dÃ©duits des chunks retenus.

> âš ï¸ Si aucun chunk pertinentÂ : rÃ©ponse courte indiquant lâ€™insuffisance du contexte.

---

## ğŸ—ï¸ Architecture interne

* **FastAPI** + **CORSMiddleware**
* **/v1/chat/completions**Â : route unique cÃ´tÃ© client
* **RAG**Â : construit un prompt enrichi par le contexte, puis appelle `UPSTREAM_MODEL_RAG`
* **FAISS**Â : persistance sur disqueÂ ; reâ€‘binding de la fonction dâ€™embedding au chargement
* **Embeddings**Â : `OrdalieTech/Solon-embeddings-large-0.1` (HuggingFace)
* **Autoâ€‘patch tokenizer**Â : ajoute `pad_token` si manquant (ex. XLMRobertaTokenizerFast)
* **Rebuild conditionnel**Â : recharge `vectorstore.pkl` ou le rÃ©gÃ©nÃ¨re si `wiki.txt` est plus rÃ©cent ou si `RAG_FORCE_REBUILD` est activÃ©

---

## ğŸ”§ Variables dâ€™environnement

| Variable | DÃ©faut | Description |
| --- | --- | --- |
| `OPENAI_API_BASE` | `http://localhost:8000/v1` | Endpoint OpenAIâ€‘compatible en amont. |
| `OPENAI_API_KEY` | `changeme` | ClÃ© API pour lâ€™amont. |
| `API_AUTH_TOKEN` | *(vide)* | Si dÃ©finiÂ : **obligatoire** en entrÃ©e via `Authorization: Bearer â€¦`. |
| `UPSTREAM_MODEL_RAG` | `gpt-4o-mini` | ModÃ¨le amont pour la gÃ©nÃ©ration RAG. |
| `UPSTREAM_MODEL_REWRITE` | `=UPSTREAM_MODEL_RAG` | ModÃ¨le amont pour la rÃ©Ã©criture et HyDE. |
| `MODEL_RAG` | `ai-rag` | Nom logique exposÃ© pour le pipeline RAG. |
| `VECTORSTORE_PATH` | `vectorstore.pkl` | Chemin du FAISS sÃ©rialisÃ© (persistÃ© sur disque). |
| `WIKI_TXT` | `wiki.txt` | Corpus brut utilisÃ© pour construire le FAISS si absent. |
| `RAG_FORCE_REBUILD` | *(vide)* | Si `1/true/on`Â : force la reconstruction du FAISS au dÃ©marrage. |
| `RAG_TOP_K` | `10` | Nombre max de chunks concatÃ©nÃ©s dans le contexte. |
| `RAG_QUERY_STRATEGY` | `rewrite+hyde` | `vanilla`, `rewrite`, `hyde` ou `rewrite+hyde`. |
| `RAG_HISTORY_WINDOW` | `6` | Nb. de messages conservÃ©s pour la rÃ©Ã©criture. |
| `RAG_TOPIC_PREFIX` | *(vide)* | PrÃ©fixe thÃ©matique forcÃ© (ex. Â«Â KubernetesÂ Â»). |
| `PORT` | `8080` | Port HTTP local. |

---

## ğŸš€ DÃ©marrage rapide

### PrÃ©requis

* PythonÂ 3.10+
* `pip` (ou `uv`, `poetry` au choix)

### Installation

```bash
python -m venv .venv && source .venv/bin/activate
pip install -U pip
pip install fastapi uvicorn[standard] requests langchain-community langchain-huggingface langchain-text-splitters faiss-cpu pydantic
# ou, pour rester alignÃ© avec ce dÃ©pÃ´tÂ :
pip install -r requirements.txt
```

> **FAISS**Â : selon lâ€™OS, vous pouvez prÃ©fÃ©rer `faiss-gpu`.

### DonnÃ©es

* Placez vos contenus dans `wiki.txt` (texte brut). Au premier dÃ©marrage sans `vectorstore.pkl`, lâ€™index sera construit et persistÃ©.

### Lancer le serveur

```bash
export OPENAI_API_BASE="https://api.openai.com/v1"           # ou votre passerelle vLLM
export OPENAI_API_KEY="sk-..."
export API_AUTH_TOKEN="my-inbound-token"
uvicorn app:app --host 0.0.0.0 --port 8080
```

---

## ğŸ§ª Appels API (exemples)

### 1) RAG (`MODEL_RAG`)

```bash
curl -s http://localhost:8080/v1/chat/completions \
  -H "Authorization: Bearer my-inbound-token" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "ai-rag",
    "messages": [
      {"role": "user", "content": "RÃ©sume les points clÃ©s du document sur la migration Kubernetes."}
    ]
  }'
```

### 2) Streaming (SSE simulÃ© cÃ´tÃ© serveur)

```bash
curl -N http://localhost:8080/v1/chat/completions \
  -H "Authorization: Bearer my-inbound-token" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "ai-rag",
    "stream": true,
    "messages": [
      {"role": "user", "content": "Donne une synthÃ¨se en 6 lignes."}
    ]
  }'
```

> En mode `stream: true`, le serveur renvoie un flux SSE crÃ©Ã© localement Ã  partir de la rÃ©ponse nonâ€‘stream de lâ€™amont.

---

## ğŸ” Authentification

* **EntrÃ©e**Â : si `API_AUTH_TOKEN` est dÃ©fini, chaque requÃªte **doit** fournir `Authorization: Bearer <token>`.
* **Sortie**Â : `OPENAI_API_KEY` est utilisÃ© cÃ´tÃ© serveur pour appeler le fournisseur amont.

---

## ğŸ—ƒï¸ Indexation & embeddings

* EmbeddingsÂ : `OrdalieTech/Solon-embeddings-large-0.1`
* SplitterÂ : `RecursiveCharacterTextSplitter(chunk_size=800, overlap=120)`
* PersistantÂ : `vectorstore.pkl`
* **Patch tokenizer**Â : si le tokenizer nâ€™a pas de `pad_token`, on utilise `eos` ou `sep` Ã  dÃ©faut, sinon ajout `[PAD]` + `resize_token_embeddings` si possible.

---

## âš™ï¸ ParamÃ©trage RAG

* **StratÃ©gies** (`RAG_QUERY_STRATEGY`)Â :

  * `vanilla`Â : pas de rÃ©Ã©criture ni HyDE
  * `rewrite`Â : rÃ©Ã©criture seule
  * `hyde`Â : HyDE seul
  * `rewrite+hyde`Â : rÃ©Ã©criture puis HyDE (par dÃ©faut)
* **Retrievers**Â :

  * `MMR`: `k=8`, `fetch_k=24`, `lambda_mult=0.5`
  * `similarity_score_threshold`: `score_threshold=0.25`, `k=12`
* **Topâ€‘K**Â : `RAG_TOP_K` (10 par dÃ©faut)
* **FenÃªtre dâ€™historique**Â : `HISTORY_WINDOW` (6 par dÃ©faut)

---

## ğŸ§¯ DÃ©pannage

* **401 Missing Authorization**Â : dÃ©finissez `API_AUTH_TOKEN` cÃ´tÃ© serveur et envoyez lâ€™enâ€‘tÃªte Bearer cÃ´tÃ© client.
* **Vectorstore introuvable**Â : fournissez `wiki.txt` au premier lancement, ou placez un `vectorstore.pkl` existant.
* **Performances embeddings**Â : selon lâ€™OS/CPU, prÃ©fÃ©rez `faiss-gpu` si GPU dispo.
* **RÃ©ponses Â«Â contexte insuffisantÂ Â»**Â : augmentez `RAG_TOP_K`, amÃ©liorez `wiki.txt`, ou baissez `score_threshold`.
* **ProblÃ¨mes tokenizer**Â : le patch auto sâ€™exÃ©cute, mais vous pouvez changer de modÃ¨le dâ€™embeddings si nÃ©cessaire.

---

## ğŸ”§ Conseils performance/qualitÃ©

* Nettoyez/structurez `wiki.txt` (titres, sÃ©parateurs) pour de meilleurs chunks
* Ajustez `chunk_size`/`overlap` si vos documents sont hÃ©tÃ©rogÃ¨nes
* Reâ€‘entraÃ®ner FAISS (reconstruire `vectorstore.pkl`) aprÃ¨s de gros changements de corpus
* Fixez `RAG_TOPIC_PREFIX` pour forcer un domaine (ex. Â«Â KubernetesÂ Â», Â«Â DSFRÂ Â», etc.)

---

## ğŸ§­ Roadmap (suggestions)

* Support **/v1/embeddings** pour publier les embeddings
* Ajout dâ€™un **reranker** (ex. crossâ€‘encoder) facultatif
* **Citations** positionnelles (lignes/offsets dans la source)
* **Batching** upstream pour rÃ©duire la latence

---

## ğŸ“„ Licence

Ã€ dÃ©finir par le propriÃ©taire du dÃ©pÃ´t (ex. MIT, Apacheâ€‘2.0).

---

## ğŸ™Œ CrÃ©dits

* FAISS â€” Facebook AI Similarity Search
* HuggingFace Transformers & sentenceâ€‘transformers
* FastAPI

---

## Structure minimale du repo

```
.
â”œâ”€â”€ app.py
â”œâ”€â”€ wiki.txt                # optionnel, pour construire FAISS au premier run
â”œâ”€â”€ vectorstore.pkl         # gÃ©nÃ©rÃ© automatiquement si absent et wiki.txt prÃ©sent
â””â”€â”€ README.md
```
